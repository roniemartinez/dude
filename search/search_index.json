{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Getting Started","text":"<p>Dude is a very simple framework for writing web scrapers using Python decorators. The design, inspired by Flask, was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax.</p> <p>Warning</p> <p>\ud83d\udea8 Dude is currently in Pre-Alpha. Please expect breaking changes.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>To install, simply run the following from terminal. Click on the annotations (+ sign) for more details.</p> Terminal <pre><code>pip install pydude #(1)\nplaywright install #(2)\n</code></pre> <ol> <li>Install <code>pydude</code> from PyPI</li> <li>Install playwright binaries for Chrome, Firefox and Webkit. See Getting Started | Playwright Python</li> </ol>"},{"location":"index.html#minimal-web-scraper","title":"Minimal web scraper","text":"<p>The simplest web scraper will look like the example below. Click on the annotations (+ sign) for more details.</p> Python <pre><code>from dude import select #(1)\n\n\n@select(css=\"a\")  #(2)\ndef get_link(element): #(3)\n    return {\"url\": element.get_attribute(\"href\")} #(4)\n</code></pre> <ol> <li>Import the <code>@select()</code> decorator</li> <li>Decorate the function <code>get_link()</code> with <code>@select()</code> and specify the selector for finding the element in the page.</li> <li>It is required that decorator functions should accept 1 argument (2 for Pyppeteer). This can be an object or a string depending on which backend was used.</li> <li>Return a dictionary with information obtained from the argument object. The dictionary can contain multiple key-value pairs or can be empty.</li> </ol> <p>The example above will get all the hyperlink elements in a page and calls the handler function <code>get_link()</code> for each element.</p>"},{"location":"index.html#how-to-run-the-scraper","title":"How to run the scraper","text":"<p>To start scraping, use any of the following options. Click on the annotations (+ sign) for more details.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --output data.json path/to/script.py #(1)\n</code></pre> <ol> <li>You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to <code>dude scrape</code> command.</li> </ol> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"]) #(1)\n</code></pre> <ol> <li>You can also use dude.run() function and run python path/to/script.py from terminal.</li> </ol> <p>The output in <code>data.json</code> should contain the actual URL and the metadata prepended with underscore.</p> <pre><code>[\n  {\n    \"_page_number\": 1,\n    \"_page_url\": \"https://dude.ron.sh/\",\n    \"_group_id\": 4502003824,\n    \"_group_index\": 0,\n    \"_element_index\": 0,\n    \"url\": \"/url-1.html\"\n  },\n  {\n    \"_page_number\": 1,\n    \"_page_url\": \"https://dude.ron.sh/\",\n    \"_group_id\": 4502003824,\n    \"_group_index\": 0,\n    \"_element_index\": 1,\n    \"url\": \"/url-2.html\"\n  },\n  {\n    \"_page_number\": 1,\n    \"_page_url\": \"https://dude.ron.sh/\",\n    \"_group_id\": 4502003824,\n    \"_group_index\": 0,\n    \"_element_index\": 2,\n    \"url\": \"/url-3.html\"\n  }\n]\n</code></pre> <p>Changing the output to <code>--output data.csv</code> should result in the following CSV content.</p> <p></p>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the terms of the GNU AGPLv3+ license.</p>"},{"location":"basic_usage.html","title":"Basic Usage","text":"<p>To use <code>dude</code>, start by importing the library.</p> Python <pre><code>from dude import select\n</code></pre> <p>A basic handler function consists of the structure below.  A handler function should accept 1 argument (element) and should be decorated with <code>@select()</code>.  The handler should return a dictionary. Click on the annotations (+ sign) for more details.</p> Python <pre><code>@select(css=\"&lt;put-your-selector-here&gt;\") # (1)\ndef handler(element): # (2)\n    ... # (3)\n    return {\"&lt;key&gt;\": \"&lt;value-extracted-from-element&gt;\"} # (4)\n</code></pre> <ol> <li><code>@select()</code> decorator.</li> <li>Function should accept 1 parameter, the element object found in the page being scraped.</li> <li>You can specify your Python algorithm here.</li> <li>Return a dictionary. This can contain an arbitrary amount of key-value pairs.</li> </ol> <p>The example handler below extracts the text content of any element that matches the CSS selector <code>.title</code>.</p> Python <pre><code>from dude import select\n\n\n@select(css=\".title\")\ndef result_title(element):\n    \"\"\"\n    Result title.\n    \"\"\"\n    return {\"title\": element.text_content()}\n</code></pre> <p>It is possible to attach a single handler to multiple selectors.</p> Python <pre><code>from dude import select\n\n\n@select(css=\"&lt;a-selector&gt;\")\n@select(selector=\"&lt;another-selector&gt;\")\ndef handler(element):\n    return {\"&lt;key&gt;\": \"&lt;value-extracted-from-element&gt;\"}\n</code></pre>"},{"location":"basic_usage.html#supported-selector-types","title":"Supported selector types","text":"<p>The <code>@select()</code> decorator does not only accept <code>selector</code> but also <code>css</code>, <code>xpath</code>, <code>text</code> and <code>regex</code>. Please take note that <code>css</code>, <code>xpath</code>, <code>text</code> and <code>regex</code> are specific and <code>selector</code> can contain any of these types.</p> Python <pre><code>from dude import select\n\n\n@select(css=\"&lt;css-selector&gt;\")     #(1)\n@select(xpath=\"&lt;xpath-selector&gt;\") #(2)\n@select(text=\"&lt;text-selector&gt;\")  #(3)\n@select(regex=\"&lt;regex-selector&gt;\") #(4)\ndef handler(element):\n    return {\"&lt;key&gt;\": \"&lt;value-extracted-from-element&gt;\"}\n</code></pre> <ol> <li>CSS Selector</li> <li>XPath Selector</li> <li>Text Selector</li> <li>Regular Expression Selector</li> </ol> <p>It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence <code>selector</code> -&gt; <code>css</code> -&gt; <code>xpath</code> -&gt; <code>text</code> -&gt; <code>regex</code>.</p>"},{"location":"basic_usage.html#how-to-run-the-scraper","title":"How to run the scraper","text":"<p>To start scraping, use any of the following options. Click on the annotations (+ sign) for more details.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --output data.json path/to/script.py #(1)\n</code></pre> <ol> <li>You can run your scraper from terminal/shell/command-line by supplying URLs, the output filename of your choice and the paths to your python scripts to <code>dude scrape</code> command.</li> </ol> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"]) #(1)\n</code></pre> <ol> <li>You can also use dude.run() function and run python path/to/script.py from terminal.</li> </ol>"},{"location":"basic_usage.html#examples","title":"Examples","text":"<p>Check out the example in examples/flat.py and run it on your terminal using the command <code>python examples/flat.py</code>.</p>"},{"location":"cli.html","title":"Command-Line Interface (CLI)","text":"CLI <pre><code>usage: dude scrape [-h] [--url URL] [--playwright | --bs4 | --parsel | --lxml | --selenium] [--headed] [--browser {chromium,firefox,webkit}] [--pages PAGES] [--output OUTPUT] [--format FORMAT]\n                   [--proxy-server PROXY_SERVER] [--proxy-user PROXY_USER] [--proxy-pass PROXY_PASS] [--follow-urls] [--save-per-page] [--ignore-robots-txt]\n                   PATH [PATH ...]\n\nRun the dude scraper.\n\noptions:\n  -h, --help            show this help message and exit\n\nrequired arguments:\n  PATH                  Path to python file/s containing the handler functions.\n  --url URL             Website URL to scrape. Accepts one or more url (e.g. \"dude scrape --url &lt;url1&gt; --url &lt;url2&gt; ...\")\n\noptional arguments:\n  --playwright          Use Playwright.\n  --bs4                 Use BeautifulSoup4.\n  --parsel              Use Parsel.\n  --lxml                Use lxml.\n  --selenium            Use Selenium.\n  --headed              Run headed browser.\n  --browser {chromium,firefox,webkit}\n                        Browser type to use.\n  --pages PAGES         Maximum number of pages to crawl before exiting (default=1). This is only valid when a navigate handler is defined.\n  --output OUTPUT       Output file. If not provided, prints into the terminal.\n  --format FORMAT       Output file format. If not provided, uses the extension of the output file or defaults to \"json\". Supports \"json\", \"yaml/yml\", and \"csv\" but can be extended using the @save() decorator.\n  --proxy-server PROXY_SERVER\n                        Proxy server.\n  --proxy-user PROXY_USER\n                        Proxy username.\n  --proxy-pass PROXY_PASS\n                        Proxy password.\n  --follow-urls         Automatically follow URLs.\n  --save-per-page       Flag to save data on every page extraction or not. If not, saves all the data at the end.If --follow-urls is set to true, this variable will be automatically set to true.\n  --ignore-robots-txt   Flag to ignore robots.txt.\n</code></pre>"},{"location":"features.html","title":"Features","text":"<ul> <li>Simple Flask-inspired design - build a scraper with decorators.</li> <li>Uses Playwright API - run your scraper in Chrome, Firefox and Webkit and leverage Playwright's powerful selector engine supporting CSS, XPath, text, regex, etc.</li> <li>Data grouping - group related results.</li> <li>URL pattern matching - run functions on matched URLs.</li> <li>Priority - reorder functions based on priority.</li> <li>Setup function - enable setup steps (clicking dialogs or login).</li> <li>Navigate function - enable navigation steps to move to other pages.</li> <li>Custom storage - option to save data to other formats or database.</li> <li>Async support - write async handlers.</li> <li>Option to use other parser backends aside from Playwright.<ul> <li>BeautifulSoup4 - <code>pip install pydude[bs4]</code></li> <li>Parsel - <code>pip install pydude[parsel]</code></li> <li>lxml - <code>pip install pydude[lxml]</code></li> <li>Selenium - <code>pip install pydude[selenium]</code></li> </ul> </li> <li>Option to follow all links indefinitely (Crawler/Spider).</li> <li>Events - attach functions to startup, pre-setup, post-setup and shutdown events.</li> <li>Option to save data on every page.</li> </ul>"},{"location":"reference.html","title":"Scraper class","text":"<p>Convenience class to easily use the available decorators.</p> Source code in <code>dude/scraper.py</code> <pre><code>class Scraper(ScraperBase):\n    \"\"\"\n    Convenience class to easily use the available decorators.\n    \"\"\"\n\n    def run(\n        self,\n        urls: Sequence[str],\n        pages: int = 1,\n        proxy: Optional[Any] = None,\n        output: Optional[str] = None,\n        format: str = \"json\",\n        follow_urls: bool = False,\n        save_per_page: bool = False,\n        ignore_robots_txt: bool = False,\n        # extra args\n        parser: str = \"playwright\",\n        headless: bool = True,\n        browser_type: str = \"chromium\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Convenience method to handle switching between different types of parser backends.\n\n        :param urls: List of website URLs.\n        :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa\n        :param proxy: Proxy settings.\n        :param output: Output file. If not provided, prints in the terminal.\n        :param format: Output file format. If not provided, uses the extension of the output file or defaults to json.\n        :param follow_urls: Automatically follow URLs.\n        :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end.\n        :param ignore_robots_txt: Flag to ignore robots.txt.\n\n        :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\" or \"selenium\"]\n        :param headless: Enables headless browser. (default=True)\n        :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\").\n        \"\"\"\n\n        logger.info(\"Scraper started...\")\n\n        if not self.scraper:\n            scraper_class: Type[ScraperBase]\n            if parser == \"bs4\":\n                from .optional.beautifulsoup_scraper import BeautifulSoupScraper\n\n                scraper_class = BeautifulSoupScraper\n            elif parser == \"parsel\":\n                from .optional.parsel_scraper import ParselScraper\n\n                scraper_class = ParselScraper\n            elif parser == \"lxml\":\n                from .optional.lxml_scraper import LxmlScraper\n\n                scraper_class = LxmlScraper\n            elif parser == \"selenium\":\n                from .optional.selenium_scraper import SeleniumScraper\n\n                scraper_class = SeleniumScraper\n            else:\n                scraper_class = PlaywrightScraper\n\n            self.scraper = scraper_class(\n                rules=self.rules,\n                groups=self.groups,\n                save_rules=self.save_rules,\n                events=self.events,\n                has_async=self.has_async,\n                requests=self.requests,\n            )\n\n        if not ignore_robots_txt:\n            logger.info(\n                f\"\"\"robots.txt is currently not ignored.\n        {\"=\" * 80}\n        Any rules/restrictions set in a website's robots.txt, will be followed by default.\n        To ignore robots.txt, add `--ignore-robots-txt` to CLI arguments or  pass `ignore_robots_txt=True` to `run()`\n        {\"=\" * 80}\"\"\",\n            )\n\n        self.scraper.run(\n            urls=urls,\n            pages=pages,\n            proxy=proxy,\n            output=output,\n            format=format,\n            follow_urls=follow_urls,\n            save_per_page=save_per_page or follow_urls,\n            ignore_robots_txt=ignore_robots_txt,\n            **{\"headless\": headless, \"browser_type\": browser_type},\n        )\n</code></pre>"},{"location":"reference.html#dude.scraper.Scraper.run","title":"<code>run(self, urls, pages=1, proxy=None, output=None, format='json', follow_urls=False, save_per_page=False, ignore_robots_txt=False, parser='playwright', headless=True, browser_type='chromium', **kwargs)</code>","text":"<p>Convenience method to handle switching between different types of parser backends.</p> <p>:param urls: List of website URLs. :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa :param proxy: Proxy settings. :param output: Output file. If not provided, prints in the terminal. :param format: Output file format. If not provided, uses the extension of the output file or defaults to json. :param follow_urls: Automatically follow URLs. :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end. :param ignore_robots_txt: Flag to ignore robots.txt.</p> <p>:param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\" or \"selenium\"] :param headless: Enables headless browser. (default=True) :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\").</p> Source code in <code>dude/scraper.py</code> <pre><code>def run(\n    self,\n    urls: Sequence[str],\n    pages: int = 1,\n    proxy: Optional[Any] = None,\n    output: Optional[str] = None,\n    format: str = \"json\",\n    follow_urls: bool = False,\n    save_per_page: bool = False,\n    ignore_robots_txt: bool = False,\n    # extra args\n    parser: str = \"playwright\",\n    headless: bool = True,\n    browser_type: str = \"chromium\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Convenience method to handle switching between different types of parser backends.\n\n    :param urls: List of website URLs.\n    :param pages: Maximum number of pages to crawl before exiting (default=1). This is only used when a navigate handler is defined. # noqa\n    :param proxy: Proxy settings.\n    :param output: Output file. If not provided, prints in the terminal.\n    :param format: Output file format. If not provided, uses the extension of the output file or defaults to json.\n    :param follow_urls: Automatically follow URLs.\n    :param save_per_page: Flag to save data on every page extraction or not. If not, saves all the data at the end.\n    :param ignore_robots_txt: Flag to ignore robots.txt.\n\n    :param parser: Parser backend [\"playwright\" (default), \"bs4\", \"parsel, \"lxml\" or \"selenium\"]\n    :param headless: Enables headless browser. (default=True)\n    :param browser_type: Playwright supported browser types (\"chromium\", \"chrome\", \"webkit\", or \"firefox\").\n    \"\"\"\n\n    logger.info(\"Scraper started...\")\n\n    if not self.scraper:\n        scraper_class: Type[ScraperBase]\n        if parser == \"bs4\":\n            from .optional.beautifulsoup_scraper import BeautifulSoupScraper\n\n            scraper_class = BeautifulSoupScraper\n        elif parser == \"parsel\":\n            from .optional.parsel_scraper import ParselScraper\n\n            scraper_class = ParselScraper\n        elif parser == \"lxml\":\n            from .optional.lxml_scraper import LxmlScraper\n\n            scraper_class = LxmlScraper\n        elif parser == \"selenium\":\n            from .optional.selenium_scraper import SeleniumScraper\n\n            scraper_class = SeleniumScraper\n        else:\n            scraper_class = PlaywrightScraper\n\n        self.scraper = scraper_class(\n            rules=self.rules,\n            groups=self.groups,\n            save_rules=self.save_rules,\n            events=self.events,\n            has_async=self.has_async,\n            requests=self.requests,\n        )\n\n    if not ignore_robots_txt:\n        logger.info(\n            f\"\"\"robots.txt is currently not ignored.\n    {\"=\" * 80}\n    Any rules/restrictions set in a website's robots.txt, will be followed by default.\n    To ignore robots.txt, add `--ignore-robots-txt` to CLI arguments or  pass `ignore_robots_txt=True` to `run()`\n    {\"=\" * 80}\"\"\",\n        )\n\n    self.scraper.run(\n        urls=urls,\n        pages=pages,\n        proxy=proxy,\n        output=output,\n        format=format,\n        follow_urls=follow_urls,\n        save_per_page=save_per_page or follow_urls,\n        ignore_robots_txt=ignore_robots_txt,\n        **{\"headless\": headless, \"browser_type\": browser_type},\n    )\n</code></pre>"},{"location":"advanced/index.html","title":"Advanced Usage","text":"<p>Dude has several useful features that allow users to control how the web scraper behaves.</p>"},{"location":"advanced/01_setup.html","title":"Setup","text":"<p>Setup handlers are very useful when performing initial actions after loading a website for the first time.  Setup handlers could perform any of the following:</p> <ol> <li>Login</li> <li>Click on dialogs buttons</li> </ol> <p>To create a Setup handler, you can pass <code>setup=True</code> parameter to <code>@select()</code> decorator.  The only difference with Setup and normal element handler is that setup functions should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details.</p> Python <pre><code>from dude import select\n\n\n@select(text=\"I agree\", setup=True) # (1)\ndef agree(element, page):\n    \"\"\"\n    Clicks \"I agree\" in order to use the website.\n    \"\"\"\n    with page.expect_navigation(): # (2)\n        element.click() # (3)\n</code></pre> <ol> <li>Finds an element containing the text \"I agree\".</li> <li>(Playwright) We expect that after clicking the element, it will navigate us to our page of interest.</li> <li>(Playwright) Perform click on the element.</li> </ol> <p>Info</p> <p>You can have multiple Setup steps, make sure to set the priority to run them in order.</p>"},{"location":"advanced/02_navigate.html","title":"Navigate","text":"<p>Navigate handlers are used to move from page to page.</p> <p>To create a Navigate handler, you can pass <code>navigate=True</code> parameter to <code>@select()</code> decorator.  Like Setup handlers, Navigate handlers should accept 2 parameters, the element matched by the selector and the Page object (or WebDriver object in Selenium). Click on the annotations (+ sign) for more details.</p> Python <pre><code>from dude import select\n\n\n@select(text=\"Next\", navigate=True) # (1)\ndef next_page(element, page):\n    \"\"\"\n    Clicks the Next button/link to navigate to the next page.\n    \"\"\"\n    with page.expect_navigation(): # (2)\n        element.click() # (3)\n</code></pre> <ol> <li>Finds an element containing the text \"Next\".</li> <li>(Playwright) We expect that after clicking the element, it will navigate us to the next page.</li> <li>(Playwright) Perform click on the element.</li> </ol> <p>Info</p> <p>You can have multiple Navigate steps, make sure to set the priority to run them in order. When having multiple Navigate steps, only the first element found will be considered and all the succeeding selectors will be skipped.</p>"},{"location":"advanced/03_grouping.html","title":"Grouping Results","text":"<p>When scraping a page containing a list of information, for example, containing URLs, titles and descriptions, it is important to know how data can be grouped together.  By default, all scraped results are grouped by <code>:root</code> which is the root document. To specify grouping, pass <code>group=&lt;selector-for-grouping&gt;</code> to <code>@select()</code> decorator.</p> <p>In the example below, the results are grouped by an element with class <code>custom-group</code>. The matched selectors should be children of this element. Click on the annotations (+ sign) for more details.</p> Python <pre><code>from dude import select\n\n\n@select(css=\".title\", group=\".custom-group\") # (1)\ndef result_title(element):\n    return {\"title\": element.text_content()}\n</code></pre> <ol> <li>Group the results by the CSS selector <code>.custom-group</code>.</li> </ol> <p>You can also specify groups by using the <code>@group()</code> decorator and passing the argument <code>selector=\"&lt;selector-for-grouping&gt;\"</code>.</p> Python <pre><code>from dude import group, select\n\n\n@group(css=\".custom-group\") # (1)\n@select(css=\".title\")\ndef result_title(element):\n    return {\"title\": element.text_content()}\n</code></pre> <ol> <li>Group the results by the CSS selector <code>.custom-group</code>.</li> </ol>"},{"location":"advanced/03_grouping.html#supported-group-selector-types","title":"Supported group selector types","text":"<p>The <code>@select()</code> decorator does not only accept <code>group</code> but also <code>group_css</code>, <code>group_xpath</code>, <code>group_text</code> and <code>group_regex</code>. Please take note that <code>group_css</code>, <code>group_xpath</code>, <code>group_text</code> and <code>group_regex</code> are specific and <code>group</code> can contain any of these types.</p> Python <pre><code>from dude import select\n\n\n@select(css=\".title\", group_css=\"&lt;css-selector&gt;\")     # (1)\n@select(css=\".title\", group_xpath=\"&lt;xpath-selector&gt;\") # (2)\n@select(css=\".title\", group_text=\"&lt;text-selector&gt;\")   # (3)\n@select(css=\".title\", group_regex=\"&lt;regex-selector&gt;\") # (4)\ndef handler(element):\n    return {\"&lt;key&gt;\": \"&lt;value-extracted-from-element&gt;\"}\n</code></pre> <ol> <li>Group CSS Selector</li> <li>Group XPath Selector</li> <li>Group Text Selector</li> <li>Group Regular Expression Selector</li> </ol> <p>It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence <code>group</code> -&gt; <code>css</code> -&gt; <code>xpath</code> -&gt; <code>text</code> -&gt; <code>regex</code>.</p> <p>Like the <code>@select()</code> decorator, the <code>@group()</code> decorator also accepts <code>selector</code>, <code>css</code>, <code>xpath</code>, <code>text</code> and <code>regex</code>. Similarly, <code>css</code>, <code>xpath</code>, <code>text</code> and <code>regex</code> are specific and <code>selector</code> can contain any of these types.</p> Python <pre><code>from dude import select\n\n\n@group(css=\"&lt;css-selector&gt;\") # (1)\n@select(selector=\"&lt;selector&gt;\") \ndef handler(element):\n    return {\"&lt;key&gt;\": \"&lt;value-extracted-from-element&gt;\"}\n</code></pre> <ol> <li>CSS Selector</li> </ol> <p>It is possible to use 2 or more of these types at the same time but only one will be used taking the precedence <code>selector</code> -&gt; <code>css</code> -&gt; <code>xpath</code> -&gt; <code>text</code> -&gt; <code>regex</code>.</p>"},{"location":"advanced/03_grouping.html#why-we-need-to-group-the-results","title":"Why we need to group the results","text":"<p>The <code>group</code> parameter or the <code>@group()</code> decorator has the advantage of making sure that items are in their correct group.  Take for example the HTML source below, notice that in the second <code>div</code>, there is no description.</p> HTML <pre><code>    &lt;div class=\"custom-group\"&gt;\n        &lt;p class=\"title\"&gt;Title 1&lt;/p&gt;\n        &lt;p class=\"description\"&gt;Description 1&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div class=\"custom-group\"&gt;\n        &lt;p class=\"title\"&gt;Title 2&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div class=\"custom-group\"&gt;\n        &lt;p class=\"title\"&gt;Title 3&lt;/p&gt;\n        &lt;p class=\"description\"&gt;Description 3&lt;/p&gt;\n    &lt;/div&gt;\n</code></pre> <p>When the group is not specified, the default grouping will be used which will result in \"Description 3\" being grouped with \"Title 2\".</p> Default Grouping <pre><code>[\n  {\n    \"_page_number\": 1,\n    // ...\n    \"description\": \"Description 1\",\n    \"title\": \"Title 1\"\n  },\n  {\n    \"_page_number\": 1,\n    // ...\n    \"description\": \"Description 3\",\n    \"title\": \"Title 2\"\n  },\n  {\n    \"_page_number\": 1,\n    // ...\n    \"title\": \"Title 3\"\n  }\n]\n</code></pre> <p>By specifying the group in <code>@select(..., group=\".custom-group\")</code>, we will be able to get a better result.</p> Specified Grouping <pre><code>[\n  {\n    \"_page_number\": 1,\n    // ...\n    \"description\": \"Description 1\",\n    \"title\": \"Title 1\"\n  },\n  {\n    \"_page_number\": 1,\n    // ...\n    \"title\": \"Title 2\"\n  },\n  {\n    \"_page_number\": 1,\n    // ...\n    \"description\": \"Description 3\",\n    \"title\": \"Title 3\"\n  }\n]\n</code></pre>"},{"location":"advanced/03_grouping.html#groups-simplify-how-you-write-your-script","title":"Groups simplify how you write your script","text":"<p>Info</p> <p>The examples below are both acceptable way to write a scraper. You have the option to choose how you write your script.</p> <p>A common way developers write scraper can be illustrated using this example below (see examples/single_handler.py for the complete script).</p> <p>While this works, it can be hard to maintain.</p> Performing all actions in one function <pre><code>from dude import select\n\n\n@select(css=\".custom-group\")\ndef result_handler(element):\n    \"\"\"\n    Perform all the heavy-lifting in a single handler.\n    \"\"\"\n    data = {}\n\n    url = element.query_selector(\"a.url\")\n    if url:\n        data[\"url\"] = url.get_attribute(\"href\")\n\n    title = element.query_selector(\".title\")\n    if title:\n        data[\"title\"] = title.text_content()\n\n    description = element.query_selector(\".description\")\n    if description:\n        data[\"description\"] = description.text_content()\n\n    return data\n</code></pre> <p>It will only require us to write 3 simple functions but is much easier to read as we don't have to deal with querying the child elements, ourselves.</p> Separate handlers with grouping <pre><code>from dude import group, select\n\n\n@select(css=\"a.url\", group=\".custom-group\")\ndef result_url(element):\n    return {\"url\": element.get_attribute(\"href\")}\n\n\n@select(css=\".title\", group=\".custom-group\")\ndef result_title(element):\n    return {\"title\": element.text_content()}\n\n\n@select(css=\".description\", group=\".custom-group\")\ndef result_description(element):\n    return {\"description\": element.text_content()}\n</code></pre>"},{"location":"advanced/03_grouping.html#when-are-group-decorator-and-group-parameter-used-by-dude","title":"When are <code>@group()</code> decorator and <code>group</code> parameter used by Dude","text":"<ol> <li>If the <code>group</code> parameter is present, it will be used for grouping.</li> <li>If the <code>group</code> parameter is not present, the selector in the <code>@group()</code> decorator will be used for grouping.</li> <li>If both <code>group</code> parameter and <code>@group()</code> decorator are not present, the <code>:root</code> element will be used for grouping.</li> </ol> <p>Info</p> <p>Use <code>@group()</code> decorator when using multiple <code>@select()</code> decorators in one function in order to reduce repetition.</p>"},{"location":"advanced/03_grouping.html#examples","title":"Examples","text":"<ul> <li>Grouping by <code>@group()</code> decorator: examples/group_decorator.py.</li> <li>Grouping by passing <code>group</code> parameter to <code>@select()</code> decorator: examples/group_in_select.py.</li> </ul>"},{"location":"advanced/04_url_pattern_matching.html","title":"URL Pattern Matching","text":"<p>In order to make a handler function to run on specific websites, a <code>url</code> pattern parameter can be passed to <code>@select()</code> decorator. The <code>url_match</code> parameter should be valid Unix shell-style wildcards (see https://docs.python.org/3/library/fnmatch.html) or custom function that returns a boolean.  The example below will only run if the URL of the current page matches <code>*.com/*</code>.</p> Python <pre><code>from dude import select\n\n\n@select(css=\".title\", url_match=\"*.com/*\")\ndef result_title(element):\n    return {\"title\": element.text_content()}\n\n\n@select(css=\"a.url\", url_match=lambda x: x.endswith(\".html\"))\ndef result_url(element):\n    return {\"url\": element.get_attribute(\"href\")}\n</code></pre>"},{"location":"advanced/04_url_pattern_matching.html#examples","title":"Examples","text":"<p>A more extensive example can be found at examples/url_pattern.py.</p>"},{"location":"advanced/05_prioritization.html","title":"Prioritization","text":"<p>Handlers are sorted based on the following sequence:</p> <ol> <li>Group</li> <li>Selector</li> <li>Priority</li> </ol> <p>If all handlers have the same priority value, they will be executed based on which handler was inserted first into the rule list. This arrangement depends on how handlers are defined inside python files and which python files was imported first. If no priority was provided to <code>@select()</code> decorator, the value defaults to 100.</p> <p>The example below makes sure that <code>result_description()</code> will be called first before <code>result_title()</code>.</p> Python <pre><code>from dude import select\n\n\n@select(css=\".title\", priority=1)\ndef result_title(element):\n    return {\"title\": element.text_content()}\n\n\n@select(css=\".description\", priority=0)\ndef result_description(element):\n    return {\"description\": element.text_content()}\n</code></pre> <p>The priority value is most useful on Setup and Navigate handlers. As an example below, the selector <code>css=\"#pnnext\"</code> will be queried first before looking for <code>text=\"Next\"</code>. Take note that if <code>css=\"#pnnext\"</code> exists, then <code>text=\"Next\"</code> will not be queried anymore.</p> Python <pre><code>from dude import select\n\n\n@select(text=\"Next\", navigate=True)\n@select(css=\"#pnnext\", navigate=True, priority=0)\ndef next_page(element, page):\n    with page.expect_navigation():\n        element.click()\n</code></pre>"},{"location":"advanced/05_prioritization.html#examples","title":"Examples","text":"<p>A more extensive example can be found at examples/priority.py.</p>"},{"location":"advanced/06_custom_storage.html","title":"Custom Storage","text":"<p>Dude currently support <code>json</code>, <code>yaml/yml</code> and <code>csv</code> formats only.  However, this can be extended to support a custom storage or override the existing formats using the <code>@save()</code> decorator. The save function should accept 2 parameters, <code>data</code> (list of dictionary of scraped data) and optional <code>output</code> (can be filename or <code>None</code>). Take note that the save function must return a boolean for success.</p> <p>The example below prints the output to terminal using tabulate for illustration purposes only.  You can use the <code>@save()</code> decorator in other ways like saving the scraped data to spreadsheets, database or send it to an API.</p> Python <pre><code>from dude import save\nimport tabulate\n\n\n@save(\"table\")\ndef save_table(data, output) -&gt; bool:\n    \"\"\"\n    Prints data to stdout using tabulate.\n    \"\"\"\n    print(tabulate.tabulate(tabular_data=data, headers=\"keys\", maxcolwidths=50))\n    return True\n</code></pre> <p>The custom storage above can then be called using any of the options below.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" path/to/script.py --format table\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"&lt;url&gt;\"], pages=2, format=\"table\")\n</code></pre>"},{"location":"advanced/06_custom_storage.html#saving-on-every-page","title":"Saving on every page","text":"<p>It is possible to call the save functions after each page. This is useful when running in spider mode to prevent lost of data. To make use of this option, the flag <code>is_per_page</code> in the <code>@save()</code> should be set to <code>True</code>.</p> Python <pre><code>@save(\"table\", is_per_page=True)\ndef save_table(data, output) -&gt; bool:\n    ...\n</code></pre> <p>To run the scraper in per-page save, pass <code>--save-per-page</code> argument.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" path/to/script.py --format table --save-per-page\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"&lt;url&gt;\"], pages=2, format=\"table\", save_per_page=True)\n</code></pre> <p>Note</p> <p>The option <code>--save-per-page</code> is best used with events to make sure that connections or file handles are opened  and closed properly. Check the examples below.</p>"},{"location":"advanced/06_custom_storage.html#examples","title":"Examples","text":"<p>A more extensive example can be found at examples/custom_storage.py and examples/save_per_page.py.</p>"},{"location":"advanced/07_the_scraper_application_class.html","title":"The Scraper Application Class","text":"<p>The decorators <code>@select()</code> and <code>@save()</code> and the function <code>run()</code> simplifies the usage of the framework. It is possible to create your own scraper application object using the example below.</p> <p>Warning</p> <p>\ud83d\udea8 This is not currently supported by the command-line interface!</p> <p>Please use the command <code>python path/to/script.py</code> to run the scraper application.</p> Python <pre><code>from dude import Scraper\n\n\napp = Scraper()\n\n\n@app.select(css=\".title\")\ndef result_title(element):\n    return {\"title\": element.text_content()}\n\n\nif __name__ == '__main__':\n    app.run(urls=[\"https://dude.ron.sh/\"])\n</code></pre>"},{"location":"advanced/07_the_scraper_application_class.html#examples","title":"Examples","text":"<p>A more extensive example can be found at examples/application.py.</p>"},{"location":"advanced/08_async.html","title":"Asynchronous Support","text":"<p>Handler functions can be converted to async.  It is not possible to mix async and sync handlers since Playwright does not support this. It is however, possible to have async and sync storage handlers at the same time since this is not connected to Playwright anymore.</p> Python <pre><code>from dude import save, select\n\n\n@select(css=\".title\")\nasync def result_title(element):\n    return {\"title\": await element.text_content()}\n\n\n@save(\"json\")\nasync def save_json(data, output) -&gt; bool:\n    ...\n    return True\n\n\n@save(\"xml\")\ndef save_xml(data, output) -&gt; bool: # (1)\n    ...\n    return True\n</code></pre> <ol> <li>Sync storage handler can be used on sync and async mode</li> </ol>"},{"location":"advanced/08_async.html#examples","title":"Examples","text":"<p>A more extensive example can be found at examples/async.py.</p>"},{"location":"advanced/09_beautifulsoup4.html","title":"BeautifulSoup4 Scraper","text":"<p>Option to use BeautifulSoup4 as parser backend instead of Playwright has been added in Release 0.2.0. BeautifulSoup4 is an optional dependency and can only be installed via <code>pip</code> using the command below.</p> Terminal <pre><code>pip install pydude[bs4]\n</code></pre>"},{"location":"advanced/09_beautifulsoup4.html#required-changes-to-your-script-in-order-to-use-beautifulsoup4","title":"Required changes to your script in order to use BeautifulSoup4","text":"<p>Instead of ElementHandle objects when using Playwright as parser backend, Soup objects are passed to the decorated functions.</p> Python <pre><code>from dude import select\n\n\n@select(css=\"a.url\")\ndef result_url(soup):\n    return {\"url\": soup[\"href\"]} # (1)\n\n\n@select(css=\".title\")\ndef result_title(soup):\n    return {\"title\": soup.get_text()} # (2)\n</code></pre> <ol> <li>Attributes can be accessed by key.</li> <li>Texts can be accessed using the <code>get_text()</code> method.</li> </ol>"},{"location":"advanced/09_beautifulsoup4.html#running-dude-with-beautifulsoup4","title":"Running Dude with BeautifulSoup4","text":"<p>You can run BeautifulSoup4 parser backend using the <code>--bs4</code> command-line argument or <code>parser=\"bs4\"</code> parameter to <code>run()</code>.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --bs4 --output data.json path/to/script.py\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"], parser=\"bs4\", output=\"data.json\")\n</code></pre>"},{"location":"advanced/09_beautifulsoup4.html#limitations","title":"Limitations","text":"<ol> <li>BeautifulSoup4 only supports CSS selector.</li> <li>Setup handlers are not supported.</li> <li>Navigate handlers are not supported.</li> </ol>"},{"location":"advanced/09_beautifulsoup4.html#examples","title":"Examples","text":"<p>Examples are can be found at examples/bs4_sync.py and examples/bs4_async.py.</p>"},{"location":"advanced/10_parsel.html","title":"Parsel Scraper","text":"<p>Option to use Parsel as parser backend instead of Playwright has been added in Release 0.5.0. Parsel is an optional dependency and can only be installed via <code>pip</code> using the command below.</p> Terminal <pre><code>pip install pydude[parsel]\n</code></pre>"},{"location":"advanced/10_parsel.html#required-changes-to-your-script-in-order-to-use-parsel","title":"Required changes to your script in order to use Parsel","text":"<p>Instead of ElementHandle objects when using Playwright as parser backend, Selector objects are passed to the decorated functions.</p> Python <pre><code>from dude import select\n\n\n@select(css=\"a.url::attr(href)\") # (1)\ndef result_url(selector):\n    return {\"url\": selector.get()} # (2)\n\n\n@select(css=\".title::text\") # (3)\ndef result_title(selector):\n    return {\"title\": selector.get()}\n</code></pre> <ol> <li>Attributes can be accessed by CSS non-standard pseudo-element, <code>::attr(name)</code>.</li> <li>Values from Selector objects can be accessed using <code>.get()</code> method.</li> <li>Texts can be accessed by CSS non-standard pseudo-element, <code>::text</code>.</li> </ol>"},{"location":"advanced/10_parsel.html#running-dude-with-parsel","title":"Running Dude with Parsel","text":"<p>You can run Parsel parser backend using the <code>--parsel</code> command-line argument or <code>parser=\"parsel\"</code> parameter to <code>run()</code>.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --parsel --output data.json path/to/script.py\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"], parser=\"parsel\", output=\"data.json\")\n</code></pre>"},{"location":"advanced/10_parsel.html#limitations","title":"Limitations","text":"<ol> <li>Setup handlers are not supported.</li> <li>Navigate handlers are not supported.</li> </ol>"},{"location":"advanced/10_parsel.html#examples","title":"Examples","text":"<p>Examples are can be found at examples/parsel_sync.py and examples/parsel_async.py.</p>"},{"location":"advanced/11_lxml.html","title":"lxml Scraper","text":"<p>Option to use lxml as parser backend instead of Playwright has been added in Release 0.6.0. lxml is an optional dependency and can only be installed via <code>pip</code> using the command below.</p> Terminal <pre><code>pip install pydude[lxml]\n</code></pre>"},{"location":"advanced/11_lxml.html#required-changes-to-your-script-in-order-to-use-lxml","title":"Required changes to your script in order to use lxml","text":"<p>Instead of ElementHandle objects when using Playwright as parser backend, Element, \"smart\" strings, etc. objects are passed to the decorated functions.</p> Python <pre><code>from dude import select\n\n\n@select(xpath='.//a[contains(@class, \"url\")]/@href') # (1)\ndef result_url(href):\n    return {\"url\": href} # (2)\n\n\n@select(css=\"a.url\")  # (3)\ndef result_url_css(element):\n    return {\"url_css\": element.attrib[\"href\"]} # (4)\n\n\n@select(css='.title')\ndef result_title(element):\n    return {\"title\": element.text} # (5)\n</code></pre> <ol> <li>Attributes can be accessed using XPath <code>@href</code>.</li> <li>When using XPath <code>@href</code> (or <code>text</code>), \"smart\" strings are returned.</li> <li>The lxml backend supports CSS selectors via <code>cssselect</code>.</li> <li>Attributes can also be accessed from lxml elements using <code>element.attrib[\"href\"]</code>.</li> <li>Text content can be accessed from lxml elements using <code>element.text</code>.</li> </ol>"},{"location":"advanced/11_lxml.html#running-dude-with-lxml","title":"Running Dude with lxml","text":"<p>You can run lxml parser backend using the <code>--lxml</code> command-line argument or <code>parser=\"lxml\"</code> parameter to <code>run()</code>.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --lxml --output data.json path/to/script.py\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"], parser=\"lxml\", output=\"data.json\")\n</code></pre>"},{"location":"advanced/11_lxml.html#limitations","title":"Limitations","text":"<ol> <li>Setup handlers are not supported.</li> <li>Navigate handlers are not supported.</li> </ol>"},{"location":"advanced/11_lxml.html#examples","title":"Examples","text":"<p>Examples are can be found at examples/lxml_sync.py and examples/lxml_async.py.</p>"},{"location":"advanced/13_selenium.html","title":"Selenium Scraper","text":"<p>Option to use Selenium as parser backend instead of Playwright has been added in Release 0.9.0. Selenium is an optional dependency and can only be installed via <code>pip</code> using the command below.</p> Terminal <pre><code>pip install pydude[selenium]\n</code></pre>"},{"location":"advanced/13_selenium.html#required-changes-to-your-script-in-order-to-use-selenium","title":"Required changes to your script in order to use Selenium","text":"<p>Instead of Playwright's <code>ElementHandle</code> objects when using Playwright as parser backend, <code>WebElement</code> objects are passed to the decorated functions.</p> Python <pre><code>from dude import select\n\n\n@select(css=\"a.url\")\ndef result_url(element, page):\n    return {\"url\": element.get_attribute(\"href\")}\n\n\n@select(css=\".title\")\ndef result_title(element, page):\n    return {\"title\": element.text}\n</code></pre>"},{"location":"advanced/13_selenium.html#running-dude-with-selenium","title":"Running Dude with Selenium","text":"<p>You can run Selenium parser backend using the <code>--selenium</code> command-line argument or <code>parser=\"selenium\"</code> parameter to <code>run()</code>.</p> TerminalPython <pre><code>dude scrape --url \"&lt;url&gt;\" --selenium --output data.json path/to/script.py\n</code></pre> <pre><code>if __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh/\"], parser=\"selenium\", output=\"data.json\")\n</code></pre>"},{"location":"advanced/13_selenium.html#limitations","title":"Limitations","text":"<ol> <li>Selenium does not support XPath 2.0, therefore not allowing regular expression.</li> </ol>"},{"location":"advanced/13_selenium.html#examples","title":"Examples","text":"<p>Examples are can be found at examples/selenium_sync.py and examples/selenium_async.py.</p>"},{"location":"advanced/14_events.html","title":"Events","text":"<p>Functions can be registered to be called on specific events which makes it possible to run custom actions like setting  up databases or calling API for authentication, and performing additional actions on page objects (can be soup, driver, selector or tree objects) like taking screenshots.</p> <p>Here is a diagram when events are being executed.</p> <p></p>"},{"location":"advanced/14_events.html#startup-event","title":"Startup Event","text":"<p>The Startup event is executed at the start of the process. </p> <p>The <code>@startup()</code> decorator can be used to register a function for startup.  This can be used to setup databases or authenticate to APIs and other possible use-cases prior to actual web scraping.</p> <pre><code>from pathlib import Path\n\nfrom dude import startup\n\nSAVE_DIR: Path\n\n\n@startup()\ndef initialize_csv():\n    global SAVE_DIR\n    SAVE_DIR = Path(__file__).resolve().parent / \"temp\"\n    SAVE_DIR.mkdir(exist_ok=True)\n</code></pre>"},{"location":"advanced/14_events.html#pre-setup-event","title":"Pre-Setup Event","text":"<p>The Pre-Setup event is executed after loading a page or getting an HTTP response.</p> <p>The <code>@pre_setup()</code> decorator can be used to register a function for pre-setup.  Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree.</p> <pre><code>import uuid\nfrom dude import pre_setup\n\n...\n\n@pre_setup()\ndef screenshot(page):\n    unique_name = str(uuid.uuid4())\n    page.screenshot(path=SAVE_DIR / f\"{unique_name}.png\")\n</code></pre>"},{"location":"advanced/14_events.html#post-setup-event","title":"Post-Setup Event","text":"<p>The Post-Setup event is executed after running the setup functions.</p> <p>The <code>@post_setup()</code> decorator can be used to register a function for post-setup.  Note that the function should accept one argument which can either be a page, driver, soup, Parsel selector or LXML tree.</p> <pre><code>import uuid\nfrom dude import post_setup\n\n...\n\n@post_setup()\ndef print_pdf(page):\n    unique_name = str(uuid.uuid4())\n    page.pdf(path=SAVE_DIR / f\"{unique_name}.pdf\")\n</code></pre>"},{"location":"advanced/14_events.html#shutdown-event","title":"Shutdown Event","text":"<p>The Shutdown event is executed before terminating the application.</p> <p>The <code>@shutdown()</code> decorator can be used to register a function for shutdown. </p> <pre><code>import shutil\n\nfrom dude import shutdown\n\n\n@shutdown()\ndef zip_all():\n    shutil.make_archive(\"images-and-pdfs\", \"zip\", SAVE_DIR)\n</code></pre>"},{"location":"advanced/15_start_requests.html","title":"@start_requests decorator","text":"<p>The <code>@start_requests</code> decorator adds an option to send custom HTTP methods (POST, PUT, PATCH, etc).</p> <p>Warning</p> <p><code>@start_requests()</code> is only supported on BeautifulSoup4, lxml and Parsel backends.</p> <p>To register custom Request objects, simply wrap a generator with <code>@start_requests</code> decorator. Click on the annotations (+ sign) for more details.</p> Python <pre><code>from dude import Request # (1)\n\n\n@start_requests()\ndef custom_requests():\n    for url in [\"https://dude.ron.sh\"]:\n        yield Request(method=\"GET\", url=url) # (2)\n\n\n@select(css=\"a.url\")\ndef result_url(soup):\n    return {\"url\": soup[\"href\"]}\n\n\nif __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[], parser=\"bs4\") # (3)\n</code></pre> <ol> <li>Import the <code>Request</code> class.</li> <li>It is necessary to specify the HTTP method.</li> <li><code>url</code> param should be set to an empty list if not needed.</li> </ol>"},{"location":"advanced/16_helper_functions.html","title":"Helper Functions","text":"<p>Here is a list of functions that can be useful for web scraping.</p>"},{"location":"advanced/16_helper_functions.html#follow_url","title":"<code>follow_url()</code>","text":"<p>This function allows adding dynamically created URLs to the list of URLs to be scraped.</p> Python <pre><code>from dude import select, follow_url\n\n\n@select(css=\".url\", group_css=\".custom-group\")\ndef url(element: BeautifulSoup) -&gt; Dict:\n\n    follow_url(element[\"href\"])\n\n    return {\"url\": element[\"href\"]}\n</code></pre>"},{"location":"advanced/16_helper_functions.html#get_current_url","title":"<code>get_current_url()</code>","text":"<p>This functions allows access to the current URL that is being scraped. It can be useful when used together with <code>follow_url()</code> function.</p> Python <pre><code>from dude import select, follow_url, get_current_url\n\n\n@select(css=\".url\", group_css=\".custom-group\")\ndef url(element: BeautifulSoup) -&gt; Dict:\n\n    follow_url(urljoin(get_current_url(), element[\"href\"]))\n\n    return {\"url\": element[\"href\"]}\n</code></pre>"},{"location":"supported_parser_backends/index.html","title":"Supported Parser Backends","text":"<p>By default, Dude uses Playwright but gives you an option to use parser backends that you are familiar with. It is possible to use parser backends like BeautifulSoup4, Parsel and lxml.</p> <p>Here is the summary of features supported by each parser backend.</p> Parser Backend SupportsSync? SupportsAsync? Selectors SetupHandler NavigateHandler Comments CSS XPath Text Regex Playwright \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BeautifulSoup4 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab Parsel \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab lxml \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \ud83d\udeab Pyppeteer \ud83d\udeab \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705 Not supported from 0.23.0 Selenium \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udeab \u2705 \u2705"},{"location":"supported_parser_backends/migrating.html","title":"Migrating Your Web Scrapers to Dude","text":"<p>Here are examples showing how web scrapers are commonly written compared to how they will be when written in Dude.</p>"},{"location":"supported_parser_backends/migrating.html#playwright","title":"Playwright","text":"<p>Example: Scrape Google search results</p> Using pure PlaywrightUsing Playwright with Dude <pre><code>import itertools\nimport json\n\nfrom playwright.sync_api import sync_playwright\n\n\ndef main(urls, output, headless, pages):\n    results = []\n\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=headless)\n        page = browser.new_page()\n\n        for url in urls:\n            page.goto(url)\n\n            # click I agree\n            with page.expect_navigation():\n                page.locator('text=\"I agree\"').click()\n\n            for page_number in range(1, pages + 1):\n                for group in page.query_selector_all(\".g\"):\n                    url_elements = group.query_selector_all(\"*css=a &gt;&gt; h3:nth-child(2)\")\n                    title_elements = group.query_selector_all(\"h3:nth-child(2)\")\n                    description_elements = group.query_selector_all(\"div[style='-webkit-line-clamp\\\\3A 2']\")\n\n                    # group together since each .g div can contain more than one set of results\n                    for url_element, title_element, description_element in itertools.zip_longest(\n                        url_elements, title_elements, description_elements\n                    ):\n                        results.append(\n                            {\n                                \"url\": url_element.get_attribute(\"href\") if url_element else None,\n                                \"title\": title_element.text_content() if title_element else None,\n                                \"description\": description_element.text_content() if description_element else None,\n                                \"page\": page_number,\n                            }\n                        )\n\n                # go to next page\n                with page.expect_navigation():\n                    page.locator(\"text=Next\").click()\n\n        browser.close()\n\n    with open(output, \"w\") as f:\n        json.dump(results, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    main(urls=[\"https://www.google.com/search?q=dude&amp;hl=en\"], output=\"data.json\", headless=False, pages=2)\n</code></pre> <pre><code>from dude import select\n\n\n@select(selector=\"*css=a &gt;&gt; h3:nth-child(2)\", group_css=\".g\")\ndef result_url(element):\n    return {\"url\": element.get_attribute(\"href\")}\n\n\n@select(css=\"h3:nth-child(2)\", group_css=\".g\")\ndef result_title(element):\n    return {\"title\": element.text_content()}\n\n\n@select(css=\"div[style='-webkit-line-clamp\\\\3A 2']\", group_css=\".g\")\ndef result_description(element):\n    return {\"description\": element.text_content()}\n\n\n@select(text=\"I agree\", setup=True)\ndef agree(element, page):\n    with page.expect_navigation():\n        element.click()\n\n\n@select(text=\"Next\", navigate=True)\ndef next_page(element, page):\n    with page.expect_navigation():\n        element.click()\n\n\nif __name__ == \"__main__\":\n    import dude\n\n    dude.run(\n        urls=[\"https://www.google.com/search?q=dude&amp;hl=en\"],\n        output=\"data.json\",\n        headless=False,\n        pages=2,\n    )\n</code></pre>"},{"location":"supported_parser_backends/migrating.html#beautifulsoup4","title":"BeautifulSoup4","text":"<p>Example: Get all links, titles and descriptions from https://dude.ron.sh</p> Using HTTPX + BeautifulSoup4Using BeautifulSoup4 with Dude <pre><code>import itertools\nimport json\n\nimport httpx\nfrom bs4 import BeautifulSoup\n\n\ndef main(urls, output):\n    results = []\n\n    with httpx.Client() as client:\n        for url in urls:\n            try:\n                response = client.get(url)\n                response.raise_for_status()\n                content = response.text\n            except httpx.HTTPStatusError as e:\n                raise\n\n            soup = BeautifulSoup(content, \"html.parser\")\n\n            for group in soup.select(\".custom-group\"):\n                url_elements = group.select(\"a.url\")\n                title_elements = group.select(\".title\")\n                description_elements = group.select(\".description\")\n\n                # group together since each .custom-group div can contain more than one set of results\n                for url_element, title_element, description_element in itertools.zip_longest(\n                    url_elements, title_elements, description_elements\n                ):\n                    results.append(\n                        {\n                            \"url\": url_element[\"href\"] if url_element else None,\n                            \"title\": title_element.get_text() if title_element else None,\n                            \"description\": description_element.get_text() if description_element else None,\n                        }\n                    )\n\n        with open(output, \"w\") as f:\n            json.dump(results, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    main(urls=[\"https://dude.ron.sh\"], output=\"data.json\")\n</code></pre> <pre><code>from dude import select\n\n\n@select(css=\"a.url\", group_css=\".custom-group\")\ndef result_url(soup):\n    return {\"url\": soup[\"href\"]}\n\n\n@select(css=\".title\", group_css=\".custom-group\")\ndef result_title(soup):\n    return {\"title\": soup.get_text()}\n\n\n@select(css=\".description\", group_css=\".custom-group\")\ndef result_description(soup):\n    return {\"description\": soup.get_text()}\n\n\nif __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh\"], parser=\"bs4\", output=\"data.json\")\n</code></pre>"},{"location":"supported_parser_backends/migrating.html#parsel","title":"Parsel","text":"<p>Example: Get all links, titles and descriptions from https://dude.ron.sh</p> Using HTTPX + ParselUsing Parsel with Dude <pre><code>import itertools\nimport json\n\nimport httpx\nfrom parsel import Selector\n\n\ndef main(urls, output):\n    results = []\n\n    with httpx.Client() as client:\n        for url in urls:\n            try:\n                response = client.get(url)\n                response.raise_for_status()\n                content = response.text\n            except httpx.HTTPStatusError as e:\n                raise\n\n            selector = Selector(content)\n            for group in selector.css(\".custom-group\"):\n                hrefs = group.css(\"a.url::attr(href)\")\n                titles = group.css(\".title::text\")\n                descriptions = group.css(\".description::text\")\n\n                # group together since each .custom-group div can contain more than one set of results\n                for href, title, description in itertools.zip_longest(hrefs, titles, descriptions):\n                    results.append(\n                        {\n                            \"url\": href.get() if href else None,\n                            \"title\": title.get() if title else None,\n                            \"description\": description.get() if description else None,\n                        }\n                    )\n\n        with open(output, \"w\") as f:\n            json.dump(results, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    main(urls=[\"https://dude.ron.sh\"], output=\"data.json\")\n</code></pre> <pre><code>from dude import select\n\n\n@select(css=\"a.url::attr(href)\", group_css=\".custom-group\")\ndef result_url(selector):\n    return {\"url\": selector.get()}\n\n\n@select(css=\".title::text\", group_css=\".custom-group\")\ndef result_title(selector):\n    return {\"title\": selector.get()}\n\n\n@select(css=\".description::text\", group_css=\".custom-group\")\ndef result_description(selector):\n    return {\"description\": selector.get()}\n\n\nif __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh\"], parser=\"parsel\", output=\"data.json\")\n</code></pre>"},{"location":"supported_parser_backends/migrating.html#lxml","title":"lxml","text":"<p>Example: Get all links, titles and descriptions from https://dude.ron.sh</p> Using HTTPX + lxml + cssselectUsing lxml with Dude <pre><code>import itertools\nimport json\n\nimport httpx\nfrom lxml import etree\n\n\ndef main(urls, output):\n    results = []\n\n    with httpx.Client() as client:\n        for url in urls:\n            try:\n                response = client.get(url)\n                response.raise_for_status()\n                content = response.text\n            except httpx.HTTPStatusError as e:\n                raise\n\n            tree = etree.HTML(text=content)\n            for group in tree.cssselect(\".custom-group\"):\n                hrefs = group.xpath('.//a[contains(@class, \"url\")]/@href')\n                titles = group.xpath('.//p[contains(@class, \"title\")]/text()')\n                descriptions = group.xpath('.//p[contains(@class, \"description\")]/text()')\n\n                # group together since each .custom-group div can contain more than one set of results\n                for href, title, description in itertools.zip_longest(hrefs, titles, descriptions):\n                    results.append(\n                        {\n                            \"url\": href,\n                            \"title\": title,\n                            \"description\": description,\n                        }\n                    )\n\n        with open(output, \"w\") as f:\n            json.dump(results, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    main(urls=[\"https://dude.ron.sh\"], output=\"data.json\")\n</code></pre> <pre><code>from dude import select\n\n\n@select(xpath='.//a[contains(@class, \"url\")]/@href', group_css=\".custom-group\")\ndef result_url(href):\n    return {\"url\": href}\n\n\n@select(xpath='.//p[contains(@class, \"title\")]/text()', group_css=\".custom-group\")\ndef result_title(text):\n    return {\"title\": text}\n\n\n@select(xpath='.//p[contains(@class, \"description\")]/text()', group_css=\".custom-group\")\ndef result_description(text):\n    return {\"description\": text}\n\n\nif __name__ == \"__main__\":\n    import dude\n\n    dude.run(urls=[\"https://dude.ron.sh\"], parser=\"lxml\", output=\"data.json\")\n</code></pre>"}]}